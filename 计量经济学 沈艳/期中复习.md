[toc]



## 双变量线性回归

### 定义

$y=\beta_0+\beta_1x+u$

y：被解释变量，回归子

x：解释变量，回归元，协变量

u：误差项，扰动项

$\beta_0,\beta_1$：回归系数（截距参数、斜率参数）



转换形式后x和y满足线性即可



### OLS（Ordinary Least Squares）估计假设

1. $E(u)=0$

不为0时可以改变$\beta_0$实现

2. 条件期望零值假定：$E(u|x)=E(u)$

说明$E(y|x)=\beta_0+\beta_1x$，即y的分布以$E(y|x)$为中心

<img src="%E6%9C%9F%E4%B8%AD%E5%A4%8D%E4%B9%A0.assets/image-20211030144108121.png" alt="image-20211030144108121" style="zoom: 25%;" />

<img src="%E6%9C%9F%E4%B8%AD%E5%A4%8D%E4%B9%A0.assets/image-20211030144134776.png" alt="image-20211030144134776" style="zoom:25%;" />

### OLS估计$\beta_0$和$\beta_1$

假定$y_i=\beta_0+\beta_1x_i+u_i$

由迭代期望定律$E(xu)=E(E(xu|x))=E(xE(u|x))=0$

$Cov(x,u)=E(xu)-E(x)E(u)=0$

得到矩阵方法

* $\frac{1}{n}\sum^n_{i=1}(y_i-\hat\beta_0-\hat\beta_1x_i)=0$
* $\frac{1}{n}\sum^n_{i=1}x_i(y_i-\hat\beta_0-\hat\beta_1x_i)=0$

第一个条件写为$\hat\beta_0=\bar y-\hat\beta_1\bar x$

**代入第二个条件得$\hat\beta_1=\frac{\sum(x_i-\bar x)(y_i-\bar y)}{\sum(x_i-\bar x)^2}=\frac{s_{xy}}{s^2_x}$**

**即样本中x和y的协方差÷x的方差**



### OLS方法

找到直线使得残差平方和最小

$\sum(\hat u_i)^2=\sum(y_i-\hat\beta_0-\hat\beta_1x_i)^2=0$

得到的式子为前面两式子×n



### OLS的代数特性

拟合值>观测值：残差<0

$y_i=\hat y_i+\hat u_i$

1. 残差和为0，样本残差均值为0：$\sum \hat u_i=\sum^n_{i=1}(y_i-\hat\beta_0-\hat\beta_1x_i)=0$

2. 回归直线过均值：$\bar y=\hat\beta_0+\hat\beta_1\bar x$

3. 残差与解释变量乘积之和为0：$\sum x_i\hat u_i=\sum^n_{i=1}x_i(y_i-\hat\beta_0-\hat\beta_1x_i)=0$

4. 预测值与残差线性不相关：$Cov(\hat y_i,\hat u_i)=0$

$\bar y=\beta_0+\beta_1\bar x+\bar u$



### 拟合优度评估

总平方和$SST=\sum(y_i-\bar y)^2$：衡量y在样本中分散程度（除以n-1得到样本方差）

解释平方和$SSE=\sum(\hat y_i-\bar y)^2$：衡量y的预测值在样本中变动

残差平方和$SSR=\sum \hat u_i^2$：衡量残差的样本变化

$SST=SSE+SSR$：y的总变动=已解释的变动SSE+未解释的变动SSR

$R^2=SSE/SST=1-SSR/SST$：衡量回归线的拟合程度，可看作是y的样本变动中可以被x解释的部分，范围为0至1





### 测量单元

y乘上c时，截距和斜率也乘c

x乘上c时，斜率除以c，截距不变



### OLS估计量的期望（无偏性）

SLR.1 参数线性：$y=\beta_0+\beta_1 x+u$

SLR.2 随机抽样：随机抽取容量为n的样本，$y_i=\beta_0+\beta_1 x_i+u$

SLR.3 零条件期望：假定$E(u|x)=0$，则$E(u_i|x_i)=0$

SLR.4 自变量样本变动：x在样本中不是常数

由以上假设得：$\beta_0$和$\beta_1$的OLS估计量的期望值等于真值（**无偏**）

因为可以得到$\hat\beta_1=\beta_1+\frac{\sum(x_i-\bar x)u_i}{\sum(x_i-\bar x)^2}$



### OLS估计量的方差

SLR.5 同方差性：$Var(u|x)=s^2$


<img src="%E6%9C%9F%E4%B8%AD%E5%A4%8D%E4%B9%A0.assets/image-20211030180933980.png" alt="image-20211030180933980" style="zoom:25%;" />

$Var(u|x)=E(u^2|x)-[E(u|x)]^2$

故有$\sigma^2=E(u^2|x)=E(u^2)=Var(u)$

$\sigma^2$：误差方差，无·条件方差

$\sigma^2=Var(y|x)=Var(u|x)$

可以得到斜率估计量方差$Var(\hat\beta_1)=\frac{\sigma^2}{s_x^2}=\frac{\sigma^2}{\sum(x_i-\bar x)^2}$

$\hat\beta_1$的标准误差：$\frac{\sigma}{(\sum(x_i-\bar x)^2)^{\frac{1}{2}}}$

截距估计量方差$$Var(\hat\beta_0)=Var(\beta_1)\frac{\sum x_i^2}{n}$$





### 误差方差的无偏估计

因为不知道误差$u_i$，所以不知道误差方差$u_i^2$，但可以用残差$\hat u_i$估计误差方差

$\hat\sigma^2=\frac{SSR}{n-2}=\frac{\sum\hat u_i^2}{n-2}$





## 多变量线性回归

### 定义

一般的多元线性回归模型：$y=\beta_0+\beta_1x_1+...+\beta_kx_k+u$



### OLS估计量

零条件期望假定：$E(u|x_1,...,x_k)=0$

最小化残差平方和的估计值$\sum(y_i-\hat\beta_0-\hat\beta_1x_{i1}-...-\hat\beta_kx_{ik})^2$

得到k+1个二阶条件：

$\sum(y_i-\hat\beta_0-\hat\beta_1x_{i1}-...-\hat\beta_kx_{ik})^2$

$\sum x_{i1}(y_i-\hat\beta_0-\hat\beta_1x_{i1}-...-\hat\beta_kx_{ik})^2$

...

$\sum x_{ik}(y_i-\hat\beta_0-\hat\beta_1x_{i1}-...-\hat\beta_kx_{ik})^2$

OLS回归线/样本回归方程（SRF）：$\hat y_i=\hat\beta_0+\hat\beta_1x_1+...+\hat\beta_kx_k$

解释：保持其他自变量固定，则$\Delta \hat y=\hat\beta_1 \Delta x_1$



特点：

* 残差项的样本均值为0

* 每个自变量和OLS残差的样本协方差为0

* $(\bar x_1,...,\bar x_k,\bar y)$在OLS回归线上



### 排除其他变量影响

对于$\hat y_i=\hat\beta_0+\hat\beta_1x_1+\hat\beta_2x_2$

回归$\hat x_1=\hat\gamma_0+\hat\gamma_1x_2$得到残差$\hat r_1$

$\hat\beta_1=\frac{\sum\hat r_{i1}y_i}{\sum \hat r_{i1}^2}$

即：y对x1,x2回归得到x1影响=先将x1对x2回归得到残差、再y对残差回归得到x1影响

变为k个解释变量时，$\hat r_1$来自x1对x2~xk回归的结果



### 单变量回归vs多变量回归

简单回归$\tilde y=\tilde\beta_0+\tilde\beta_1x_1$，多元回归$\hat y_=\hat\beta_0+\hat\beta_1x_1+\hat\beta_2x_2$

$\tilde\beta_1=\hat\beta_1$当且仅当：

* $\hat\beta_2=0$（x2对y没有局部效应）

* 或者x1和x2在样本中不相关

  

存在关系$\tilde\beta_1=\hat\beta_1+\hat\beta_2\tilde\delta_1$，其中$\tilde\delta_1$是x2对x1简单回归的斜率系数

即$\tilde\beta_1=\hat\beta_1+\hat\beta_2\frac{\sum(x_1-\bar {x_1})(x_2-\bar{x_2})}{\sum(x_1-\bar {x_1})^2}$



$j=0,1,...,k$

设$\hat\beta_j$为用全部x回归的估计量，$\tilde\beta_j$为除$x_k$外的x回归的估计量

设$\tilde\delta_j$为$x_k$对$x_1,...,x_{k-1}$回归中$x_j$的斜率系数

则$\tilde\beta_j=\hat\beta_j+\hat\beta_k\tilde\delta_j$



单元对x1的估计值=多元对x1的估计值当且仅当：

* $x_2\sim x_k$OLS系数均为0
* $x_1$和$x_2\sim x_k$无关



### OLS估计量的期望值

MLR.1 对参数线性：$y=\beta_0+\beta_1x_1+...+\beta_kx_k+u$，

MLR.2 对于$y_i=\beta_0+\beta_1x_{i1}+...+\beta_kx_{ik}+u_i$，可以用容量为n的随机样本$\{(x_{i1},...,x_{ik};y_i): i=1,...,n\}$，i为第i个观察值，j为第j个回归变量

MLR.3 零条件均值：$E(u|x_{i1},x_{i2},...,x_{ik})$，此时称所有x为外生的

MLR.4 不存在完全共线性：x不为常数，且不存在线性关系

（会使得OLS估计量分母为0）



定理3.1：给定上述假设，$E(\hat\beta_j)=\beta_j$，即OLS估计量是总体参数的无偏估计



### 太多或太少变量

过度设定：没有局部效应的自变量放在模型里

不影响无偏性，影响方差

设定不足：遗漏了本属于真实模型的变量

使得OLS有偏



假设真实模型为$y=\beta_0+\beta_1x_1+\beta_2x_2+u$，但估计为$\tilde y=\tilde\beta_0+\tilde\beta_1x_1$

$\tilde\beta_1=\frac{\sum(x_{i1}-\bar x_1)y_i}{\sum(x_{i1}-\bar x_1)^2}$

代入得$E(\tilde\beta_1)=\beta_1+\beta_2\frac{\sum(x_{i1}-\bar x_1)x_{i2}}{\sum(x_{i1}-\bar x_1)^2}=\beta_1+\beta_2\tilde\delta_1$

$E(\tilde\beta_1)>\beta_1$称为上偏，否则下偏

无偏：

* $\beta_2=0$即x2不属于模型
* 或者x1和x2不相关



如果$Corr(x_1,x_2)>0$与$\beta_2$（x2和y相关性）同正负，偏误为正，反方向则为负

一般不知道$\beta_2$，所以不知道$Corr(x1,x2)>0$符号

如果省略对y有局部效应的变量，且该变量和至少一个x相关，则估计量都有偏



### OLS估计量的方差

MLR.5 同方差性：$Var(u|x_1,...,x_k)=s^2$

解释变量取值不影响误差的条件方差

则有$Var(u|\bold x)=Var(y|\bold x)=\sigma^2$

MLR.1~MLR.5成为高斯-马尔可夫假定



定理3.2：$Var(\hat\beta_j)=\frac{\sigma^2}{SST_j(1-R_j^2)}$

其中$SST_j=\sum(x_{ij}-\bar x_j)^2$，$R_j^2$是$x_j$向所有其他x回归得到的$R^2$

受误差方差、总样本变异、解释变量之间线性相关关系决定

* 误差方差越大：OLS方差大，噪音多，与样本容量无关
* 总样本变异越大：OLS方差小，可以通过增加样本容量来增大样本方差
* 多重共线性（$R_j^2$接近1）越大：其他x可以解释该变量程度高，方差大
  * 可以舍弃一些变量/收集更多数据
  * 发现多重共线性：$VIF_J=1/(1-R_j^2)$，超过5或10表示多重共线性



对于$\tilde y=\tilde \beta_0+\tilde \beta_1x_1$，估计的方差$Var(\tilde \beta_1)=\frac{\sigma^2}{SST_1}$

当x1和x2的互相关性为0时，$Var(\tilde \beta_1)=Var(\hat\beta_1)$

否则$Var(\tilde\beta_1)<Var(\hat\beta_1)$

漏掉x2后，偏差变大，方差变小。喜欢包含x2，因为可以增大样本容量抵消多重共线性



* 模型设定不足：偏差大，方差小

* 模型过度设定：偏差小，方差大

  

用观察到的残差项$\hat u_i$来构造误差方差的估计$\hat\sigma^2=\frac{\sum\hat u_i^2}{n-k-1}$

自由度$df=n-k-1$，观察点个数-被估参数个数

（k+1个限制条件推导OLS估计，给定n-k-1个残差可以推出剩余k+1个残差）



定理3.3：误差$E(\hat\sigma^2)=\sigma^2$

$Var(\hat\beta_j）=\frac{\hat\sigma^2}{SST_j(1-R_j^2)}$



### OLS的有效性：高斯马尔科夫定理

OLS是最优线性无偏估计量（BLUE）

最优：方差最小

$\hat\beta_1=\sum w_{i1}y_{i}$，其中$w_{i1}=\frac{x_{i1}-\bar x_1}{\sum(x_{i1}-\bar x_1)^2}$



### R2

增加解释变量，$R^2$上升，除非完全共线，因为残差平方和不会增加

$R^2$增加不一定表明新加入的变量提高了模型拟合度

调整过的$R^2$：$\bar R^2=1-\frac{n-1}{n-k-1}\frac{SSR}{SST}$

* 始终比原$R^2$小
* 增加解释变量，可能增加可能减少
* SSR可正可负

$R^2$不能告诉我们选择的回归元是不是最好/是否应该加入一个回归元。后者由对y的局部效应决定





## 多元回归的推断

### OLS估计量的样本分布

MLR.6 正态：假设u与$x_1,...x_k$独立，且u服从均值为0、方差为s2的正态分布

MLR.1~6成为经典线性模型假定



$\hat\beta_j$是误差的线性组合，所以服从正态分布，均值为$\beta_j$，方差为$Var(\hat\beta_j)$



### t检验：对单个总体参数的假设检验

零假设：被检验的假设

尽量减小第一类错误：零假设为真时拒绝零假设（弃真）

显著性水平：第一类错误概率。对应于临界值

拒绝域：使得零假设被拒绝的检验统计量范围

t检验步骤：

* 写出零假设和替代假设
* 选择显著水平和对应的临界值
* 根据样本计算t统计量
* 比较t统计量和临界值，决定是否拒绝零假设



### t检验—双边检验，原假设真值是0（$\beta_j=0$）

t绝对值大于临界值c时拒绝0假设。根据$\alpha/2$计算临界值，$\alpha=0.05$时c为n-k-1自由度的t分布的97.5位数

$H_0:\beta_j=0$，$H_1:\beta_j\neq0$

拒绝0假设称为“$x_j$在$\alpha$水平下显著”

<img src="%E6%9C%9F%E4%B8%AD%E5%A4%8D%E4%B9%A0.assets/image-20211102155502512.png" alt="image-20211102155502512" style="zoom: 67%;" />

构造t统计量$t_{\hat\beta_j}=\hat\beta_j/se(\hat\beta_j)$

t分布自由度增大时，趋近于标准正态分布



### t检验—单边检验

$H_1:\beta_j>0$（拒绝$H_0$如果$t_{\hat\beta_j}>c$），或$\beta_j<0$（拒绝$H_0$如果$t_{\hat\beta_j}<-c$）

在显著性水平$\alpha$上找到n-k-1自由度的t分布的1-$\alpha$分位数，即为临界值

<img src="%E6%9C%9F%E4%B8%AD%E5%A4%8D%E4%B9%A0.assets/image-20211102160636375.png" alt="image-20211102160636375" style="zoom:67%;" />



对于$H_0:\beta_j=a_j$，需要更一般的t统计量$t=(\hat\beta_j-a_j)/se(\hat\beta_j)$



p值：算得的t统计量作为临界值，零假设被拒绝的最小显著性水平

$p=P(|T|>|t|)$

p越小越可以拒绝零假设

可以解释为：零假设成立时，观察到算得的t值的概率

用p值（统计显著性，由t决定）、系数大小来权衡，决定解释变量对被解释变量的边际影响



### 置信区间

置信区间：使用随机样本构造取值区间，使得真值在给定概率（置信度）下属于此区间

若总体服从的正态分布均值为$\mu$，方差为1，则样本服从的正态分布均值为$\mu$，方差为$1/n$，可以用一个特定的随机样本计算置信区间

由$P(-1.96<\frac{\bar Y-\mu}{1/\sqrt{n}}<1.96)=0.95$得：

置信区间$[\bar Y-1.96/\sqrt{n}, \bar Y+1.96\sqrt{n}]$包含$\mu$的概率是95%

$[\bar y-1.96/\sqrt{n}, \bar y+1.96\sqrt{n}]$的概率不一定

双边置信区间可用于双边检验。若零假设下$\mu_0$不在95%置信区间内，则拒绝原假设，否则不能拒绝



$(1-\alpha)%$的置信区间定义为$\hat\beta_j\pm c*se(\hat\beta_j)$，其中c为$t_{n-k-1}$分布的$1-\frac{\alpha}{2}$分位数



进行双尾假设检验，零假设$H_0: \beta_j=a_j$当且仅当$a_j$不在95%置信区间

零假设在5%显著水平上被拒绝





### F检验

$H_0:\beta_{k-q+1}=0,...\beta_k=0$

计算F统计量$F=\frac{(SSR_r-SSR_{ur})/q}{SSR_{ur}/(n-k-1)}$，r为约束回归，ur为无约束回归

F始终大于0，因为约束回归的SSR大于等于无约束回归SSR

F衡量无约束变为约束的SSR增量

$q=df_r-df_{ur}$即衡量的x的个数，$n-k-1=df_{ur}$，一般q远小于n-k-1

如果F>C，拒绝原假设，成为x联合显著



* 估计无约束模型，得到SSR和自由度
* 计算约束模型中排除的变量个数q
* 估计约束模型，得到SSR
* 计算F统计量



用R2计算

$F=\frac{(R_{ur}^2-R_r^2)/q}{(1-R_{ur}^2)/(n-k-1)}$



F用于检验单个约束。对于双边约束，$F=t^2$









